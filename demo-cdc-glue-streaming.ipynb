{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			},
			"source": [
				"# Demo - Debezium + AWS Glue + Apache Iceberg"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  1/ Configuring AWS Glue session\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%session_id_prefix cdc-debezium-kinesis-iceberg\n",
				"%glue_version 3.0\n",
				"%idle_timeout 30\n",
				"%number_of_workers 6\n",
				"%streaming\n",
				"%%configure \n",
				"{\n",
				"  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
				"  \"--datalake-formats\": \"iceberg\"\n",
				"}"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  2/ Defining parameters. Replace \\<bucket_name> with your bucket name and \\<stream_name> with your Amazon Kinesis stream name."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"bucket_name = \"<bucket_name>\" \n",
				"stream_name = \"<stream_name>\" #\"debezium-demo.DemoDBZ.MYTABLE\"\n",
				"\n",
				"catalog_name = \"glue_catalog\"\n",
				"bucket_prefix = \"cdc\"\n",
				"database_name = \"demo_cdc_debezium\"\n",
				"table_name = \"mytable\"\n",
				"warehouse_path = f\"s3://{bucket_name}/{bucket_prefix}\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"####  3/ Importing libraries, starting Spark session and Glue context."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"import sys\n",
				"import json\n",
				"import boto3\n",
				"\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"from awsglue import DynamicFrame\n",
				"\n",
				"from pyspark.sql import SparkSession\n",
				"from pyspark.context import SparkContext\n",
				"from pyspark.conf import SparkConf\n",
				"\n",
				"spark = SparkSession.builder \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\") \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
				"    .config(f\"spark.sql.catalog.{catalog_name}.lock.table\", \"iceberg_metastore\") \\\n",
				"    .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
				"    .getOrCreate()\n",
				"\n",
				"glueContext = GlueContext(spark)\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"####  4/ Droping table on AWS Glue catalog if exists."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"query = f\"\"\"\n",
				"DROP TABLE IF EXISTS {catalog_name}.{database_name}.{table_name}\n",
				"\"\"\"\n",
				"spark.sql(query)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"####  5/ Creating database on AWS Glue catalog if not exists."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"query = f\"\"\"\n",
				"CREATE DATABASE IF NOT EXISTS {database_name}\n",
				"\"\"\"\n",
				"spark.sql(query)   \n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"####  6/ Creating data frame (DF) from Kinesis Stream."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"stream_arn= boto3.client('kinesis').describe_stream(StreamName=stream_name)[\"StreamDescription\"][\"StreamARN\"]\n",
				"\n",
				"conn_={\n",
				"    \"typeOfData\": \"kinesis\",\n",
				"    \"streamARN\": stream_arn,\n",
				"    \"classification\": \"json\",\n",
				"    \"startingPosition\": \"earliest\",#latest, \"2023-09-25T19:35:00-03:00\" Glue 4.0+\n",
				"    \"inferSchema\": \"true\"\n",
				"}\n",
				"\n",
				"kinesis_data = glueContext.create_data_frame.from_options(\n",
				"    connection_type=\"kinesis\",\n",
				"    connection_options=conn_,\n",
				"    transformation_ctx=\"kinesis_data\",\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"####  7/ Sampling input stream for interactive development"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"options = {\n",
				"    \"pollingTimeInMs\": \"10000\",\n",
				"    \"windowSize\": \"5 seconds\",\n",
				"}\n",
				"\n",
				"dyf = glueContext.getSampleStreamingDynamicFrame(kinesis_data, options, None)\n",
				"#To run streaming applications instead of just sampling, change getSampleStreaming to forEachBatch method \n",
				"\n",
				"print(dyf.count())"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"When the sampled Dynamic Frame is empty (0), the polling time could not be enough to process the records it ingested. Increase poolingTimeInMs value and try again."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  8/ Reading DF schema"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"df2 = DynamicFrame.fromDF(dyf.toDF().select(\"payload\"), glueContext, 'df2')\n",
				"df2.printSchema()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  9/ (Optional) Stream data filterd by date\n",
				"\n",
				"##### Because getSampleStreamingDynamicFrame method was configured with the value \"earlist\" for the \"startingPosition\" parameter, every time you run the method it will read all valid data records. So, it can be useful apply filters on data frame, specially if you are interested in removing some data related to other tests. If it's your case, uncomment the lines below."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"#from pyspark.sql import functions as F\n",
				"#df3 = df2.toDF().withColumn(\"datetime_utc\", F.to_utc_timestamp(F.from_unixtime(F.col(\"payload.ts_ms\")/1000,'yyyy-MM-dd HH:mm:ss'),'UTC')).withColumn(\"epoch\",F.col(\"payload.ts_ms\"))\n",
				"#df3.sort(\"epoch\").show(n=100,truncate=30)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  10/ Attention here! Creating temporary view from streaming data.\n",
				"##### Only if you performed last step (optional), uncomment the second line below and replace XXXXXXXX by your desired interval."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"df4 = df2.toDF()\n",
				"#df4 = df3.filter(df3.epoch >= XXXXXXXX)\n",
				"df4.createOrReplaceTempView(\"DEBEZIUM0\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  11/ Simplifying the reading of streaming data"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"sql1 = \"\"\"\n",
				"SELECT\n",
				"    payload.ts_ms as ts_ms\n",
				"    ,payload.op as op\n",
				"    ,payload.after as after\n",
				"    ,payload.before as before\n",
				"    ,payload.after.DATA as data\n",
				"    ,payload.after.DATA_ID as id\n",
				"    ,payload.after.*\n",
				"FROM DEBEZIUM0\n",
				"WHERE payload.op is not null\n",
				"ORDER BY ts_ms\n",
				"\"\"\"\n",
				"dfX = spark.sql(sql1)\n",
				"dfX.show(n=100,truncate=200)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  12/ Creating iceberg table from temporary view"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"query = f\"\"\"\n",
				"CREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.{table_name}\n",
				"USING iceberg\n",
				"TBLPROPERTIES ('table_type'='ICEBERG', 'format-version'='2')\n",
				"LOCATION '{warehouse_path}'\n",
				"AS SELECT payload.after.* FROM DEBEZIUM0 where 1=0\n",
				"\"\"\"\n",
				"spark.sql(query)\n",
				"\n",
				"spark.catalog.listTables(database_name)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  13/ Reading iceberg metadata"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"spark.sql(f\"select * from {catalog_name}.{database_name}.{table_name}.history\").show(truncate=False)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"spark.sql(f\"select * from {catalog_name}.{database_name}.{table_name}.snapshots\").show(truncate=False,vertical=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"spark.sql(f\"select * from {catalog_name}.{database_name}.{table_name}.files\").show(truncate=False,vertical=True)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  14/ UPSERT (UPDATE OR INSERT) operation to add or update data into iceberg table\n",
				"##### Only the last updated data will be inserted on iceberg table, this can improve a lot the time spend to process data, therefore cost savings."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"sql1 = \"\"\"\n",
				"with cte as\n",
				"(\n",
				"SELECT\n",
				"    max(payload.ts_ms) as ts_ms\n",
				"    ,payload.after.DATA_ID\n",
				"FROM DEBEZIUM0\n",
				"WHERE payload.op is not null and payload.op != \"d\"\n",
				"GROUP BY payload.after.DATA_ID\n",
				")\n",
				"select distinct \n",
				"    d.payload.after.DATA\n",
				"    ,d.payload.after.DATA_ID\n",
				"from cte as c \n",
				"inner join DEBEZIUM0 as d\n",
				"    on c.ts_ms = d.payload.ts_ms\n",
				"    and c.DATA_ID = payload.after.DATA_ID\n",
				"\"\"\"\n",
				"\n",
				"df_upsert = spark.sql(sql1)\n",
				"\n",
				"df_upsert.createOrReplaceTempView(\"UPSERT0\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  15/ Reading UPSERT data"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"spark.sql(\"SELECT * FROM UPSERT0\").show(n=100)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  16/ Performing the UPSERT operation on iceberg table"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"spark.sql(f\"\"\"MERGE INTO {catalog_name}.{database_name}.{table_name} t\n",
				"            USING UPSERT0 u ON u.DATA_ID = t.DATA_ID\n",
				"            WHEN MATCHED THEN UPDATE SET *\n",
				"            WHEN NOT MATCHED THEN INSERT *\n",
				"            \"\"\")\n",
				"\n",
				"glueContext.create_data_frame.from_catalog(database=f'{database_name}', table_name=f'{table_name}').show(n=100)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  17/ DELETE operation to remove data from iceberg table"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"sql1 = \"\"\"\n",
				"with cte as\n",
				"(\n",
				"SELECT\n",
				"    max(payload.ts_ms) as ts_ms\n",
				"    ,COALESCE(payload.after.DATA_ID,payload.before.DATA_ID) as DATA_ID\n",
				"FROM DEBEZIUM0\n",
				"WHERE payload.op is not null\n",
				"GROUP BY COALESCE(payload.after.DATA_ID,payload.before.DATA_ID)\n",
				")\n",
				"select distinct c.DATA_ID\n",
				"from cte as c \n",
				"inner join DEBEZIUM0 as d\n",
				"    on c.ts_ms = d.payload.ts_ms\n",
				"    and c.DATA_ID = COALESCE(payload.after.DATA_ID,payload.before.DATA_ID)\n",
				"where payload.op = \"d\"\n",
				"\"\"\"\n",
				"\n",
				"if df2.toDF().select(F.col(\"payload.before\")).dtypes[0][1] != 'string':\n",
				"    df_delete = spark.sql(sql1)\n",
				"    df_delete.createOrReplaceTempView(\"DELETE0\")\n",
				"else:\n",
				"    print(\"Nothing to DELETE.\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  18/ Reading DELETE data"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"if df2.toDF().select(F.col(\"payload.before\")).dtypes[0][1] != 'string':\n",
				"    spark.sql(\"SELECT * FROM DELETE0\").show(n=100)\n",
				"else:\n",
				"    print(\"Nothing to DELETE.\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  19/ Performing the DELETE operation on iceberg table"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"if df2.toDF().select(F.col(\"payload.before\")).dtypes[0][1] != 'string':\n",
				"    spark.sql(f\"\"\"MERGE INTO {catalog_name}.{database_name}.{table_name} t\n",
				"                USING DELETE0 d ON d.DATA_ID = t.DATA_ID\n",
				"                WHEN MATCHED THEN DELETE\n",
				"                \"\"\")\n",
				"\n",
				"glueContext.create_data_frame.from_catalog(database=f'{database_name}', table_name=f'{table_name}').show(n=100)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  20/ Reading iceberg metadata"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"spark.sql(f\"select * from {catalog_name}.{database_name}.{table_name}.snapshots\").show(truncate=False,vertical=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"spark.sql(f\"select * from {catalog_name}.{database_name}.{table_name}.history\").show(truncate=False)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"####  21/ Stopping Glue session\n",
				"##### To avoid additional costs, when you finish testing, execute command below to close the Glue session"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%stop_session"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
